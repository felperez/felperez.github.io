---
title: 'Statistical properties in hyperbolic dynamics, part 2'
date: 2019-06-04
permalink: /posts/2019/06/blog-post-6/
tags:
  - probability theory

  - measure theory

  - limit laws

  - dynamical systems

  - hyperbolic dynamics

  - transfer operator
---

[Previous post](/posts/2019/06/blog-post-5/)

This is the second post of a series of 4 posts based on the lectures at the [Houston Summer School on Dynamical Systems 2019](https://www.math.uh.edu/dynamics/school/school2019/) on Statistical properties in hyperbolic dynamics, given by Matthew Nicol, Andrew Török and William Ott. These notes are heavily edited with added comments, examples and explanations. Any mistake is of my responsibility. Some of the notation has been changed for consistency.

## Lecture 2

In this lecture we will explore in more depth the transfer operator method, as well as Gordin's martingale approximation method.

Recall the setting from the previous lecture: let $T\colon X\to X$ be a measure preserving transformation of the probability space $(X,\mu)$. For a regular enough observable $\phi\colon X \to\mathbb{R}$, we constructed the time series $X\_n = \phi\circ T^n$. We are interested in the limit laws of this time series. For this, we will write the observable as

$$
\phi = \psi + g - g\circ T
$$

for functions $\psi,g$, so that when we compute the Birkhoff sums, we obtaining

$$
\sum_{j=1}^n \phi\circ T^j = \sum_{j=1}^n \psi\circ T^j + g(x) - g\circ T^{n+1}(x).
$$

The idea is that the sum on the right hand side is a reverse martingale, and the two remaining terms are controllable.

## Probability recap

Let $(\Omega,\mathbb{P})$ be a probility space with sigma-algebra $\mathcal{B}$ and $\mathcal{F}\subset\mathcal{B}$ a sub sigma-algebra. If $Y$ is a random variable with finite expectation, define the *conditional expectation* of $Y$ with respect to $\mathcal{F}$, denoted by $\mathbb{E}(Y | \mathcal{F})$ to be the unique (up to zero measure sets) random variable $Z$ with the following properties:
1. $Z$ is $\mathcal{F}$-measurable
2. for every set $A\in\mathcal{F}$,

$$
\int_A Z d\mathbb{P} = \int_A Y d\mathbb{P}.
$$

The existence of $\mathbb{E}(Y \| \mathcal{F})$ can be proved using [Radon-Nikodym's theorem](https://en.wikipedia.org/wiki/Radon–Nikodym_theorem) We list now some easy to check properties of the conditional expectation:

**Properties:**
1. $\mathbb{E}(\cdot \| \mathcal{F})$ is a linear operator,
2. $\mathbb{E}(\cdot \| \mathcal{F})$ is a positive operator,
3. $\int\_{\Omega} \mathbb{E}(X\|\mathcal{F}) dP = \int\_{\Omega} X dP$ for every integrable random variable $X$,
4. if $\mathcal{F}_1\subset\mathcal{F}_2\subset\mathcal{B}$ then $\mathbb{E}(\mathbb{E}(X\| \mathcal{F_2})\|\mathcal{F}_1) = \mathbb{E}(X\|\mathcal{F}_1)$,
5. if $X\in\mathcal{F}$ and $\mathcal{B}(X),\mathbb{E}(XY)<\infty$, then $\mathbb{E}(XY\| \mathcal{F}) = X\mathbb{E}(Y\|\mathcal{F})$,
6. if $X$ is $\mathcal{F}$-measurable, then $\mathbb{E}(X\|\mathcal{F}) = X$.

We can now give a definition of a martingale. A sequence of sub sigma-algebras $(\mathcal{F}_n)$ is a *filtration* if $\mathcal{F}_n\subset\mathcal{F}\_{n+1}$ for all $n$. A stochastic process $(M_n)$ is a *martingale* with respect to the filtration $(\mathcal{F}_n)$ if
1. $\mathbb{E}(\|M_n\|) \lt \infty$,
2. $M_n$ is $\mathcal{F}_n$-measurable,
3. $\mathbb{E}(M_{n+1}\|\mathcal{F}_n) =  M_n$.

In this setting, we call $X_n = M\_{n+1} - M_n$ a *martingale difference*.

**Remark:** if $M_n = X_1 + \dots + X_n$ and we take the filtration $\mathcal{F}_n = \sigma(X_1,\dots,X_n)$, then $(M_n)$ is a martingale if and only if $\mathbb{E}(X\_{n+1}\| \mathcal{F}_n) = 0$.

## Transfer operator revisited

Recall the definition of the transfer operator $P$ as the dual of the Koopman operator:

$$
\int_X \phi\cdot\psi\circ T d\mu = \int_X P\phi \cdot \psi d\mu
$$

for observables $\phi\in L^1$ and $\psi\in L^\infty$. In this section we will discuss how we can express the action of the transfer and the Koopman operators in terms of conditional expectations. Let $\mathcal{F}_n = T^{-n}\mathcal{B}$, where $\mathcal{B}$ is the sigma-algebra of the underlying phase space $X$.

**Proposition:** for every $\phi\in L^1$ we have:
1. $\mathbb{E}(\phi \| T^{-1}\mathcal{B}) = UP\phi = (P\phi)\circ T$ and similarly $\mathbb{E}(\phi \| T^{-i}\mathcal{B}) = U^iP^i\phi$.
2. $PU\phi = \phi$ and $P^iU^i \phi = \phi$.

**Proof:** for the first part, we have to show that $UP\phi$ is $T^{-1}\mathcal{B}$-measurable and that

$$
\int_A UP\phi d\mu = \int_A \phi d\mu
$$

for every set $A\in T^{-1}\mathcal{B}$. Note that $(P\phi\circ T)^{-1}(B) = T^{-1}(P\phi)^{-1}(B)$ for every set $B\in\mathcal{B}$, thus the measurability condition holds. Using invariance and duality, we have

$$
\int_{T^{-1}A} UP\phi d\mu = \int_X (P\phi)\circ T 1_{T^{-1}A} d\mu = \int_X (P\phi)\circ T 1_A\circ Td\mu
$$

$$
\qquad = \int_X (P\phi) 1_A d\mu = \int_X \phi 1_A\circ T d\mu = \int_{T^{-1}A}\phi d\mu.
$$

This shows the first property. The property for the iterates follows from noticing that $P^i$ is the transfer operator associated to the map $T^i$. For the second property, note that for $g\in L^\infty$, we have

$$
\int_X (PU\phi)gd\mu = \int_X U\phi g\circ T d\mu = \int_X \phi\circ T g\circ T d\mu = \int_X \phi g d\mu.
$$

A standard approximation argument finishes the proof of the claim. $\square$

## Gordin's method

Suppose we have a Banach space $\mathcal{B}\_{\alpha} \subset L^2$ with norm $\| \cdot \|\_{\alpha}$ such that for observables $\phi$ with $\int \phi d\mu = 0$, we have $\|P^n\phi \|\_{\alpha}\leq \rho(n)\| \phi \|\_{\alpha}$ with $r(n)$ summable. Define $g = \sum_{n=1}^\infty P^n \phi$ where $\phi\in\mathcal{B}\_{\alpha}$, and define $\psi = \phi + g - g\circ T$. We have that $UP\psi = \mathbb{E}(\psi\|T^{-1}\mathcal{B})$, hence if we show that $P\psi = 0$, then $\mathbb{E}(\psi \| T^{-1}\mathcal{B}) = 0$. Note that the sum defining $g$ converges and $g\in\mathcal{B}\_{\alpha}$. Then

$$
P\psi = P\phi + P\sum_{n=1}^\infty P^n\phi - PUg = P\phi + \sum_{n=2}^\infty P^n\phi - g = 0.
$$

Let $\mathcal{F}_j = T^{-j}\mathcal{B}$ and $X_j = \psi\circ T^j$. Then

$$
\mathbb{E}(X_j | \mathcal{F}_{j+1}) = \mathbb{E}(\psi\circ T^j | T^{-j-1}\mathcal{B}) = U^{j+1}P^{j+1}U^j \psi = U^{j+1}P\psi = 0
$$

so we have a reverse martingale difference. Define $G_j = T^{-(n-j)\mathcal{B}}$ and $Y_j = \psi\circ T^{n-j}$ so $Y_j$ is $G_j$- measurable. The filtration $G_j$ is increasing, and if we define $M_j = \sum_{k=1}^n Y_k$, then $(M_n)$ is a martingale with respect to the filtration $(G_j)$, which leaves us in the setting of martingales differences.

## Limit laws

Now that we know how to write the Birkhoff sums of an observable in terms of a martingale plus a coboundary, we can transfer the limit theorems known for martingales to our dynamical setting.

**Theorem (Azuma-Hoeffding):** Let $(M_n)$ be a martingale with respect to a filtration $(\mathcal{F}_n)$ so then $X_n = M\_{n+1}-M_n$ is a martingale difference. Suppose that $\|X_n\|\leq C$ almost surely for a constant $C > 0$. Then for $\alpha > 0$ and $n\geq 1$

$$
\mathbb{P}\left(\dfrac{M_n}{n}> \alpha\right) \leq \exp\left(\dfrac{-\alpha^2n}{2C^2}\right).
$$

**Corollary:** if $\phi$ is a Lipschitz observable with zero mean, we have

$$
\mu\left(\dfrac{1}{n}\sum_{k=1}^n \phi\circ T^k > \alpha\right) \leq \exp(-\tau n)
$$

for a constant $\tau > 0$ and every $n\geq 1$.

The proof follows from the approximation method above taking $(M_n)$ to be the Birkhoff sums. The boundedness condition follows from the regularity of the observable. The same idea can be used to prove other limit laws, for instance Cuny and Merlevede proved [here](https://arxiv.org/abs/1209.3677) the following functional law of iterated logarithms:

**Theorem:** Suppose $\phi \in L^2$ and $\phi\_n = \phi\circ T^n$ for $T$ ergodic. Let $\mathcal{G}\_j$ a filtration so that $\phi_n$ is adapted to it and $\mathbb{E}(\phi\_{n} \| \mathcal{G}\_{n+1})=0$ almost surely. Then it is possible to find centered Gaussian random variables $(Z\_j)$ with $\mathbb{E}(Z\_{k}^{2})=\mathbb{E}(\phi\_{1}^{2})$ such that

$$
\sup_{1 \leq k \leq n}\left|\sum_{i=1}^{k} \phi_{i}-\sum_{i=1}^{k} Z_{i}\right|=o(\sqrt{n \log \log n})
$$

almost surely.

[Next post](/posts/2019/06/blog-post-8/)
