---
title: 'Statistical properties in hyperbolic dynamics'
date: 2019-06-03
permalink: /posts/2019/05/blog-post-5/
tags:
  - probability theory

  - measure theory

  - limit laws

  - dynamical systems

  - hyperbolic dynamics

  - transfer operator
---

This is the first post of a series of 4 posts based on the lectures at the [Houston Summer School on Dynamical Systems 2019](https://www.math.uh.edu/dynamics/school/school2019/) on Statistical properties in hyperbolic dynamics, given by Matthew Nicol, Andrew Török and William Ott. These notes are heavily edited with added comments, examples and explanations. Any mistake is of my responsibility. Some of the notation has been changed for consistency.

## Lecture 1

The goal of these lectures is to understand statistical properties of dynamical systems by using some tools from probability theory. For this, we will introduce first some language and notation.

### Random variables

Let $(\Omega,P)$ be a probability space. A *random variable* is a measurable funcion $Z\colon\Omega\to\mathbb{R}$ (in the context of dynamical systems, we will call random variables *observables*). A stochastic process is a sequence of random variables $(Z_k)$ in the same probability space. The distribution function of a random variable $\phi$ is the function $F_Z\colon(-\infty,\infty)\to[0,1]$ given by

$$
F_Z(t) = \mathbb{P}(X\leq t)
$$

**Example:** a random variable $Z$ has *normal distribution* (denoted by $Z\sim \mathcal{N}(\mu,\sigma^2)$) if

$$
F_Z(t) = \dfrac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^t \exp\left(\frac{-(x-\mu)^2}{2\sigma^2}\right)dx.
$$

A stochastic process $(Z_k)$ is *stationary* if for every $n\geq 0$ and every sequence $A_1,\dots,A_k$ of measurable sets, we have

$$
P(X_1\in A_1,\dots,X_k\in A_k) = P(X\in A_{1+n},\dots,X_k\in A_{k+n}).
$$

This means that the joint distribution of the random variables does not change over time.

### Dynamical systems

We turn now to our main interest: dynamical systems. Suppose $T\colon X\to X$ is a probability measure preserving map of the probability space $(X,\mu)$, that is $\mu(T^{-1}(A)=\mu(A))$ for all measurable sets $A$.

Let $\phi$ be an observable. If we iterate the map, we can generate a time series $\{X_n=  \phi\circ T^n\}$. If $T$ is measure preserving, then this time series is a stationary stochastic process. In particular, the $X_n$ are identically distributed, but they are not necessarily independent. Recall $X_1,X_2$ are independent if

$$
P(X_1\in A_1 , X_2\in A_2) = P(X_1\in A_1)P(X_2\in A_2).
$$

What statistical properties does the time series $(X_n)$ posses?

One of the main results of Ergodic Theory, is the well celebrated Birkhoff's Ergodic Theory, which generalizes the probabilistic Law of Large Numbers. Recall the system $T$ is ergodic with respect to the measure $\mu$ if $T^{-1}A = A$ implies that $\mu(A) = 0$ or $\mu = 1$.

*Birkhoff's Ergodic theorem:* if $T$ is ergodic and $\phi$ is integrable, then for $\mu$ almost every $x$

$$
	\dfrac{1}{n}\sum_{j=0}^{n-1}\phi\circ T^j(x) \to \int_X \phi d\mu.
$$

We can see [here](https://www.math.uh.edu/~climenha/blog-posts/ergodic-theorem.pdf) how the Ergodic theorem generalizes the LLN.

Once we know the almost sure behavior of the averages of the time series, we can ask for a second order limit law, namely, a Central Limit Theorem. Recall its formulation in the probabilistic setting:

*Central limit theorem:* if $(X_n)$ are iid and $\mathbb{E}(X_1^2)<\infty$, then

$$
\dfrac{\sum_{j=1}^n X_j -n\mathbb{E}(X_1)}{\sqrt{n\var(X_j)}} \to^d \mathcal{N}(0,1)
$$

If $T$ is 'chaotic' (hyperbolic) we expect $\phi\circ T^k$ to be roughly independent of $\phi$ for large $k$. We quantify this using correlations:

For two observables $\phi,\psi: X\to \mathbb{R}$, we define their correlation by
\begin{align*}
	C_{\phi,\psi}(n) = \int_X \phi\cdot \psi\circ T^n d\mu - \int_X \phi d\mu \int\psi d\mu.
\end{align*}

Autocorrelation: take $\psi = \phi$.

We can measure the decay of correlations with respect to any measure, not necessarily an invariant measure.

In general if we want to get results on the decay of correlations, we need to assume some regularity of the observables, usually $\phi,\psi$ lying in some 'regular' Banach space: $\phi\in\mathcal{B}_\alpha,\psi\in\mathcal{B}_\beta$, where we can take $\mathcal{B}_\alpha = L^1$, $\mathcal{B}_\beta = L^\infty$, etc.

3 main techniques to prove decay of correlations:
\begin{enumerate}
\item Transfer operator
\item Cone method, using Hilbert metric
\item Coupling
\end{enumerate}
